{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ad89d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x86528 and 346112x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 192\u001b[39m\n\u001b[32m    189\u001b[39m optimizer.zero_grad()\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m    195\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MdSourav/DNN/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MdSourav/DNN/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mYOLOv1.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Flatten the output for the fully connected layers\u001b[39;00m\n\u001b[32m    135\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    137\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Reshape to match the grid size (batch_size, 13, 13, 5)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MdSourav/DNN/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MdSourav/DNN/yolo/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MdSourav/DNN/yolo/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (2x86528 and 346112x4096)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Dataset Class for WIDERFACE\n",
    "class WiderFaceDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transform=None, img_size=(208, 208)):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size  # Default is 416x416\n",
    "        self.image_paths = []\n",
    "        self.annotations = []\n",
    "\n",
    "        # List all image files in the images directory\n",
    "        image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "            label_file = os.path.splitext(image_file)[0] + '.txt'  # Match the image name with label file\n",
    "            label_path = os.path.join(annotations_dir, label_file)\n",
    "\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                # Process each annotation in the file\n",
    "                boxes = []\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    num_faces = int(parts[0])\n",
    "\n",
    "                    for i in range(num_faces):\n",
    "                        x_center = float(parts[2 + 4*i])  # normalized x-center\n",
    "                        y_center = float(parts[3 + 4*i])  # normalized y-center\n",
    "                        width = float(parts[4 + 4*i])     # normalized width\n",
    "                        height = float(parts[5 + 4*i])    # normalized height\n",
    "                        boxes.append([x_center, y_center, width, height])\n",
    "\n",
    "                self.image_paths.append(image_path)\n",
    "                self.annotations.append(boxes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize image to 416x416 for YOLOv1 input\n",
    "        image = cv2.resize(image, self.img_size)\n",
    "\n",
    "        # Convert to float32 and normalize to range [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Convert image to a tensor\n",
    "        image = torch.tensor(image).permute(2, 0, 1)  # Convert to CxHxW format\n",
    "\n",
    "        boxes = self.annotations[idx]\n",
    "\n",
    "        # Normalize boxes to grid size (YOLOv1 uses 13x13 grid for 416x416 input)\n",
    "        grid_size = 13\n",
    "        target = np.zeros((grid_size, grid_size, 5))  # 5: 4 for box + 1 for confidence\n",
    "\n",
    "        for box in boxes:\n",
    "            x_center, y_center, width, height = box\n",
    "            grid_x = int(x_center * grid_size)\n",
    "            grid_y = int(y_center * grid_size)\n",
    "\n",
    "            target[grid_y, grid_x, 0] = x_center  # Box center X\n",
    "            target[grid_y, grid_x, 1] = y_center  # Box center Y\n",
    "            target[grid_y, grid_x, 2] = width     # Width\n",
    "            target[grid_y, grid_x, 3] = height    # Height\n",
    "            target[grid_y, grid_x, 4] = 1         # Confidence (since we have faces)\n",
    "\n",
    "        # Convert target to tensor with correct dtype\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# YOLOv1 Architecture\n",
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, img_size=416, grid_size=13):\n",
    "        super(YOLOv1, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.grid_size = grid_size  # Grid size (e.g., 13x13)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(192, 128, 1, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the output after all convolutional and pooling layers\n",
    "        self._initialize_conv_output_size()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 4096)  # Dynamic flattened size\n",
    "        self.fc2 = nn.Linear(4096, self.grid_size * self.grid_size * 5)  # Output for 13x13 grid with 5 values (box + confidence)\n",
    "\n",
    "    def _initialize_conv_output_size(self):\n",
    "        # Create a dummy input to calculate the output size after convolutional layers\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, self.img_size, self.img_size)  # Batch size of 1, RGB image\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            self.flattened_size = dummy_output.numel()\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self._forward_conv(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Reshape to match the grid size (batch_size, 13, 13, 5)\n",
    "        x = x.view(-1, self.grid_size, self.grid_size, 5)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Training Setup (CPU Only)\n",
    "device_cpu = torch.device('cpu')  # Use only CPU\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2  # Reduced batch size\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Dataset Paths\n",
    "train_images_dir = '/home/cse/Documents/MdSourav/DNN/Assignment7/WIDER Face Dataset/split_data/train/images'\n",
    "train_annotations_dir = '/home/cse/Documents/MdSourav/DNN/Assignment7/WIDER Face Dataset/split_data/train/labels'\n",
    "val_images_dir = '/home/cse/Documents/MdSourav/DNN/Assignment7/WIDER Face Dataset/split_data/val/images'\n",
    "val_annotations_dir = '/home/cse/Documents/MdSourav/DNN/Assignment7/WIDER Face Dataset/split_data/val/labels'\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = WiderFaceDataset(\n",
    "    images_dir=train_images_dir, \n",
    "    annotations_dir=train_annotations_dir, \n",
    "    img_size=(208, 208)  # Resize to 208x208\n",
    ")\n",
    "val_dataset = WiderFaceDataset(\n",
    "    images_dir=val_images_dir, \n",
    "    annotations_dir=val_annotations_dir, \n",
    "    img_size=(208, 208)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = YOLOv1()\n",
    "model = model.to(device_cpu)  # Store model on CPU\n",
    "\n",
    "criterion = nn.MSELoss()  # Using MSELoss for simplicity (you can implement a custom YOLO loss function)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device_cpu)  # Keep images on CPU for training\n",
    "        targets = targets.to(device_cpu)  # Keep targets on CPU for training\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'yolov1_face_detector_cpu.pth')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Evaluate the model on validation set (Optional)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss = 0.0\n",
    "    for images, targets in val_loader:\n",
    "        images = images.to(device_cpu)  # Keep images on CPU for evaluation\n",
    "        targets = targets.to(device_cpu)  # Keep targets on CPU for evaluation\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
