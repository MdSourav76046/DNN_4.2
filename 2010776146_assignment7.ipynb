{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdSourav76046/DNN_4.2/blob/main/2010776146_assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWp6DapFF210",
        "outputId": "4bcc69be-13f9-45a7-ec01-bb0d46f4a924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.162)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install ultralytics opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKuuNTRN_rv6",
        "outputId": "c5da0e74-f055-43df-e3b8-6c3a4298dde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/wider-face-a-face-detection-benchmark\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mksaad/wider-face-a-face-detection-benchmark\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99oM1C-MGgsc",
        "outputId": "4f8be7bc-d64e-4d56-f1b9-ec9d90cd2488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Converted 12880 images for split 'train'.\n",
            " Converted 3226 images for split 'val'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/10]: 100%|██████████| 805/805 [03:53<00:00,  3.45it/s, loss=6.18e+13]\n",
            "Epoch [2/10]: 100%|██████████| 805/805 [03:52<00:00,  3.45it/s, loss=2.26e+13]\n",
            "Epoch [3/10]: 100%|██████████| 805/805 [03:51<00:00,  3.47it/s, loss=9.39e+12]\n",
            "Epoch [4/10]: 100%|██████████| 805/805 [03:51<00:00,  3.47it/s, loss=4.76e+12]\n",
            "Epoch [5/10]: 100%|██████████| 805/805 [03:52<00:00,  3.46it/s, loss=2.8e+12]\n",
            "Epoch [6/10]: 100%|██████████| 805/805 [03:51<00:00,  3.48it/s, loss=1.29e+12]\n",
            "Epoch [7/10]: 100%|██████████| 805/805 [03:51<00:00,  3.48it/s, loss=9.81e+11]\n",
            "Epoch [8/10]: 100%|██████████| 805/805 [03:52<00:00,  3.47it/s, loss=4.61e+11]\n",
            "Epoch [9/10]: 100%|██████████| 805/805 [03:53<00:00,  3.45it/s, loss=2.95e+11]\n",
            "Epoch [10/10]: 100%|██████████| 805/805 [03:52<00:00,  3.47it/s, loss=1.66e+11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as 'yolov1_face.pt'\n"
          ]
        }
      ],
      "source": [
        "# === YOLOv1 Face Detector Full Implementation ===\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "# === Config ===\n",
        "S = 7  # grid size\n",
        "B = 2  # number of bounding boxes\n",
        "C = 1  # number of classes (face)\n",
        "IMG_SIZE = 256\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# === Paths ===\n",
        "ANNOTATION_DIR = \"/root/.cache/kagglehub/datasets/mksaad/wider-face-a-face-detection-benchmark/versions/4/wider_face_split/wider_face_split\"\n",
        "TRAIN_IMG_DIR = \"/root/.cache/kagglehub/datasets/mksaad/wider-face-a-face-detection-benchmark/versions/4/WIDER_train/WIDER_train/images\"\n",
        "VAL_IMG_DIR = \"/root/.cache/kagglehub/datasets/mksaad/wider-face-a-face-detection-benchmark/versions/4/WIDER_val/WIDER_val/images\"\n",
        "\n",
        "TRAIN_ANNOTATION_FILE = os.path.join(ANNOTATION_DIR, \"wider_face_train_bbx_gt.txt\")\n",
        "VAL_ANNOTATION_FILE = os.path.join(ANNOTATION_DIR, \"wider_face_val_bbx_gt.txt\")\n",
        "\n",
        "# === Dataset storage ===\n",
        "YOLOv1_DATASET = {\n",
        "    \"train\": [],\n",
        "    \"val\": []\n",
        "}\n",
        "\n",
        "def convert_for_yolov1(txt_path, image_root, split):\n",
        "    with open(txt_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    idx = 0\n",
        "    total_images = 0\n",
        "\n",
        "    while idx < len(lines):\n",
        "        rel_image_path = lines[idx].strip()\n",
        "        face_count = int(lines[idx + 1].strip())\n",
        "        full_image_path = os.path.join(image_root, rel_image_path)\n",
        "\n",
        "        if not os.path.exists(full_image_path):\n",
        "            idx += 2 + face_count\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(full_image_path)\n",
        "        if img is None:\n",
        "            idx += 2 + face_count\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        label_matrix = torch.zeros((S, S, 5 + C))\n",
        "\n",
        "        for i in range(face_count):\n",
        "            x, y, bw, bh = map(int, lines[idx + 2 + i].strip().split()[:4])\n",
        "\n",
        "            xc = (x + bw / 2) / w\n",
        "            yc = (y + bh / 2) / h\n",
        "            nw = bw / w\n",
        "            nh = bh / h\n",
        "\n",
        "            i_cell, j_cell = int(yc * S), int(xc * S)\n",
        "            if i_cell >= S or j_cell >= S:\n",
        "                continue\n",
        "\n",
        "            x_cell = xc * S - j_cell\n",
        "            y_cell = yc * S - i_cell\n",
        "\n",
        "            if label_matrix[i_cell, j_cell, 0] == 0:\n",
        "                label_matrix[i_cell, j_cell, 0:5] = torch.tensor([1, x_cell, y_cell, nw, nh])\n",
        "                label_matrix[i_cell, j_cell, 5] = 1  # class one-hot (face)\n",
        "\n",
        "        YOLOv1_DATASET[split].append((full_image_path, label_matrix))\n",
        "        total_images += 1\n",
        "        idx += 2 + face_count\n",
        "\n",
        "    print(f\" Converted {total_images} images for split '{split}'.\")\n",
        "\n",
        "# === Run conversion ===\n",
        "convert_for_yolov1(TRAIN_ANNOTATION_FILE, TRAIN_IMG_DIR, \"train\")\n",
        "convert_for_yolov1(VAL_ANNOTATION_FILE, VAL_IMG_DIR, \"val\")\n",
        "\n",
        "# === Dataset Class ===\n",
        "class YOLOv1Dataset(Dataset):\n",
        "    def __init__(self, dataset_list, transform=None):\n",
        "        self.data = dataset_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label_matrix = self.data[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "\n",
        "        return img, label_matrix\n",
        "\n",
        "# === Example: Create Dataloaders ===\n",
        "train_dataset = YOLOv1Dataset(YOLOv1_DATASET['train'])\n",
        "val_dataset = YOLOv1Dataset(YOLOv1_DATASET['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "class YOLOv1(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):  # C=1 for face class\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.S = S  # grid size\n",
        "        self.B = B  # number of bounding boxes per grid\n",
        "        self.C = C  # number of classes (1 for face)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(192, 128, kernel_size=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # 4 conv blocks (all maintaining 512 channels)\n",
        "            *[nn.Sequential(\n",
        "                nn.Conv2d(512, 256, kernel_size=1),\n",
        "                nn.LeakyReLU(0.1),\n",
        "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "                nn.LeakyReLU(0.1)\n",
        "            ) for _ in range(4)],\n",
        "\n",
        "            # Transition to 1024 channels\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Final blocks (all 1024 channels)\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            # Last two conv layers\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.AdaptiveAvgPool2d((self.S, self.S))\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, S * S * (C + B * 5))  # final output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.fc(x)\n",
        "        return x.view(-1, self.S, self.S, self.C + self.B * 5)\n",
        "\n",
        "\n",
        "# === Loss Function (Simplified YOLO Loss) ===\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction='sum')\n",
        "        self.S, self.B, self.C = S, B, C\n",
        "        self.lambda_coord = 5\n",
        "        self.lambda_noobj = 0.5\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        obj = target[..., 4].unsqueeze(-1)\n",
        "\n",
        "        coord_loss = self.lambda_coord * self.mse(preds[..., 0:2] * obj, target[..., 0:2] * obj)\n",
        "        size_loss = self.lambda_coord * self.mse(preds[..., 2:4] * obj, target[..., 2:4] * obj)\n",
        "        conf_loss = self.mse(preds[..., 4:5] * obj, target[..., 4:5] * obj) + \\\n",
        "                    self.lambda_noobj * self.mse(preds[..., 4:5] * (1 - obj), target[..., 4:5] * (1 - obj))\n",
        "        class_loss = self.mse(preds[..., 5:] * obj, target[..., 5:] * obj)\n",
        "\n",
        "        return coord_loss + size_loss + conf_loss + class_loss\n",
        "\n",
        "# === Training Loop ===\n",
        "def train(model, dataloader, optimizer, criterion, epochs=30):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs)\n",
        "            loss = criterion(preds, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# === Run Training ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = YOLOv1().to(device)\n",
        "\n",
        "criterion = YoloLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{10}]\")\n",
        "    for imgs, targets in loop:\n",
        "        imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# === Save Model ===\n",
        "torch.save(model.state_dict(), \"yolov1_face.pt\")\n",
        "print(\"✅ Model saved as 'yolov1_face.pt'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}